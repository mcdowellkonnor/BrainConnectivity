{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730e71cb",
   "metadata": {},
   "source": [
    "# Per Node Permutation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "\n",
    "from scipy.stats import loguniform, lognorm\n",
    "from sklearn.svm import LinearSVC, SVC, NuSVC\n",
    "from sklearn.model_selection import (\n",
    "    LeaveOneOut,\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.warn = warn\n",
    "\n",
    "data_df = None\n",
    "if os.path.isfile(\"data/data.pkl\"):\n",
    "    data_df = pd.read_pickle(\"data/data.pkl\")\n",
    "else:\n",
    "    pass\n",
    "data_df.head()\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "# Define X and Y\n",
    "ad_hc_df = data_df[\n",
    "    data_df[\"Diagnosis\"].isin([\"AD\", \"HC\"]) & (data_df[\"Harmonized\"].notna())\n",
    "].copy()\n",
    "tbi_df = data_df[\n",
    "    data_df[\"Diagnosis\"].isin([\"NEG\", \"POS\"]) & (data_df[\"Harmonized\"].notna())\n",
    "].copy()\n",
    "\n",
    "X_ad_hc = np.vstack(ad_hc_df[\"EVC\"].values)\n",
    "X_tbi = np.vstack(tbi_df[\"EVC\"].values)\n",
    "\n",
    "y_ad_hc = ad_hc_df[\"Diagnosis\"].map({\"AD\": 1, \"HC\": 0}).values\n",
    "y_tbi = tbi_df[\"Diagnosis\"].map({\"POS\": 1, \"NEG\": 0}).values\n",
    "\n",
    "# Ensure X is standard scaled to start\n",
    "X_ad_hc = StandardScaler().fit_transform(X_ad_hc)\n",
    "X_tbi = StandardScaler().fit_transform(X_tbi)\n",
    "\n",
    "SPECIFIC_NODES = [2, 7, 83, 86, 120, 167]\n",
    "# SPECIFIC_NODES = list(range(X_ad_hc.shape[1]))\n",
    "\n",
    "# Model Definitions\n",
    "# clf = NuSVC(\n",
    "#     probability=True,\n",
    "#     random_state=RANDOM_STATE,\n",
    "#     cache_size=2000,\n",
    "#     class_weight=\"balanced\"\n",
    "# )\n",
    "# clf = LinearSVC(\n",
    "#     penalty=\"l2\",\n",
    "#     class_weight=\"balanced\",\n",
    "#     random_state=RANDOM_STATE,\n",
    "#     verbose=1,\n",
    "#     max_iter=10000,\n",
    "# )\n",
    "clf = SVC(\n",
    "    kernel=\"sigmoid\",\n",
    "    class_weight=\"balanced\",\n",
    "    probability=True,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "svc_params = {\n",
    "    \"C\": loguniform(10**-4, 10**4),\n",
    "}\n",
    "\n",
    "use_grid_search = True\n",
    "gridsearch = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=svc_params,\n",
    "    n_iter=1000,\n",
    "    cv=LeaveOneOut(),\n",
    "    refit=True,\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    "    error_score=\"raise\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "\n",
    "def loocv_classification(mdl, X, y):\n",
    "    loo = LeaveOneOut()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for train_index, test_index in loo.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        clf = clone(mdl)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred.append(clf.predict(X_test))\n",
    "        y_true.append(y_test)\n",
    "\n",
    "    return accuracy_score(y_true, y_pred), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a model for AD/HC\n",
    "if use_grid_search:\n",
    "    gridsearch.fit(X_ad_hc[:, SPECIFIC_NODES], y_ad_hc)\n",
    "    best_model_ad = gridsearch.best_estimator_\n",
    "    pd.DataFrame(gridsearch.cv_results_).to_csv(\"out/ad_hc_grid.csv\")\n",
    "else:\n",
    "    best_model_ad = clone(clf).fit(X_ad_hc[:, SPECIFIC_NODES], y_ad_hc)\n",
    "\n",
    "print(\"=== GRID SEARCH TEST ===\")\n",
    "score_ad, y_pred = loocv_classification(\n",
    "    best_model_ad, X_ad_hc[:, SPECIFIC_NODES], y_ad_hc\n",
    ")\n",
    "print(np.c_[y_ad_hc, np.array(y_pred).flatten()])\n",
    "print(classification_report(y_ad_hc, y_pred))\n",
    "print(score_ad)\n",
    "print(best_model_ad.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36474c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test on holdout using parameters from grid\n",
    "X_train_ad_holdout, X_test_ad_holdout, y_train_ad_holdout, y_test_ad_holdout = (\n",
    "    train_test_split(\n",
    "        X_ad_hc[:, SPECIFIC_NODES], y_ad_hc, test_size=0.33, random_state=RANDOM_STATE\n",
    "    )\n",
    ")\n",
    "ad_holdout_scaler = StandardScaler()\n",
    "X_train_ad_holdout = ad_holdout_scaler.fit_transform(X_train_ad_holdout)\n",
    "X_test_ad_holdout = ad_holdout_scaler.transform(X_test_ad_holdout)\n",
    "ad_holdout_clf = SVC(**best_model_ad.get_params())  # Leakage?, data -> params -> mdl\n",
    "ad_holdout_clf.fit(X_train_ad_holdout, y_train_ad_holdout)\n",
    "print(\"=== HOLDOUT TEST ===\")\n",
    "print(\n",
    "    classification_report(y_test_ad_holdout, ad_holdout_clf.predict(X_test_ad_holdout))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d84adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a model for TBI+/TBI-\n",
    "print(\"=== OPTIMIZED MODEL ===\")\n",
    "if use_grid_search:\n",
    "    gridsearch = clone(gridsearch)\n",
    "    gridsearch.fit(X_tbi[:, SPECIFIC_NODES], y_tbi)\n",
    "    pd.DataFrame(gridsearch.cv_results_).to_csv(\"out/tbi_grid.csv\")\n",
    "    best_model_tbi = gridsearch.best_estimator_\n",
    "else:\n",
    "    best_model_tbi = clone(clf).fit(X_tbi[:, SPECIFIC_NODES], y_tbi)\n",
    "\n",
    "score_tbi, y_pred = loocv_classification(\n",
    "    best_model_tbi, X_tbi[:, SPECIFIC_NODES], y_tbi\n",
    ")\n",
    "print(classification_report(y_tbi, y_pred))\n",
    "print(score_tbi)\n",
    "print(best_model_tbi.get_params())\n",
    "\n",
    "print(\"\\n=== PARAMETER ONLY MODEL ===\")\n",
    "parameter_only_tbi = SVC(**best_model_ad.get_params())\n",
    "# parameter_only_tbi.fit(X_tbi[:, SPECIFIC_NODES], y_tbi)\n",
    "score_param_tbi, y_pred = loocv_classification(\n",
    "    parameter_only_tbi, X_tbi[:, SPECIFIC_NODES], y_tbi\n",
    ")\n",
    "print(classification_report(y_tbi, y_pred))\n",
    "print(score_param_tbi)\n",
    "print(parameter_only_tbi.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc265d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the AD/HC model onto the TBI+/TBI- and obtain Classification Report\n",
    "y_pred = best_model_ad.predict(X_tbi[:, SPECIFIC_NODES])\n",
    "print(classification_report(y_tbi, y_pred))\n",
    "score_transfer = best_model_ad.score(X_tbi[:, SPECIFIC_NODES], y_tbi)\n",
    "print(score_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the AD/HC model onto the AD/HC SCALED TBI+/TBI- and obtain Classification Report\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_ad_hc[:, SPECIFIC_NODES])\n",
    "X_tbi_scaled = scaler.transform(X_tbi[:, SPECIFIC_NODES])\n",
    "y_pred = best_model_ad.predict(X_tbi_scaled)\n",
    "print(classification_report(y_tbi, y_pred))\n",
    "score_transfer_scaled = best_model_ad.score(X_tbi[:, SPECIFIC_NODES], y_tbi)\n",
    "print(score_transfer_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3903a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation of Nodes\n",
    "# Each iteration, we will change the nodes used for X\n",
    "from src.helper import tqdm_joblib\n",
    "\n",
    "NUM_PERMUTATIONS = 100\n",
    "NUM_NODES = len(SPECIFIC_NODES)\n",
    "\n",
    "\n",
    "def run_permutation(seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm_nodes = rng.choice(X_ad_hc.shape[1], size=NUM_NODES, replace=False)\n",
    "\n",
    "    perm_ad_model = SVC(**best_model_ad.get_params())\n",
    "    perm_ad_score, _ = loocv_classification(\n",
    "        perm_ad_model, X_ad_hc[:, perm_nodes], y_ad_hc\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    ad_hc_scaled = scaler.fit_transform(X_ad_hc[:, perm_nodes])\n",
    "    tbi_scaled = scaler.transform(X_tbi[:, perm_nodes])\n",
    "\n",
    "    perm_transfer_score = perm_ad_model.fit(ad_hc_scaled, y_ad_hc).score(\n",
    "        X_tbi[:, perm_nodes], y_tbi\n",
    "    )\n",
    "    perm_scaled_transfer_score = perm_ad_model.fit(ad_hc_scaled, y_ad_hc).score(\n",
    "        tbi_scaled, y_tbi\n",
    "    )\n",
    "\n",
    "    perm_tbi_model = SVC(**best_model_tbi.get_params())\n",
    "    perm_tbi_score, _ = loocv_classification(\n",
    "        perm_tbi_model, X_tbi[:, perm_nodes], y_tbi\n",
    "    )\n",
    "\n",
    "    perm_param_model = SVC(**best_model_ad.get_params())\n",
    "    perm_param_score, _ = loocv_classification(\n",
    "        perm_param_model, X_tbi[:, perm_nodes], y_tbi\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"Nodes\": perm_nodes,\n",
    "        \"AD Score\": perm_ad_score,\n",
    "        \"TBI Score\": perm_tbi_score,\n",
    "        \"Transfer Score\": perm_transfer_score,\n",
    "        \"Scaled Transfer Score\": perm_scaled_transfer_score,\n",
    "        \"AD Param-only Score\": perm_param_score,\n",
    "    }\n",
    "\n",
    "\n",
    "with tqdm_joblib(tqdm(range(NUM_PERMUTATIONS), desc=\"Permutations\")) as progress_bar:\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(run_permutation)(i) for i in range(NUM_PERMUTATIONS)\n",
    "    )\n",
    "\n",
    "perm_df = pd.DataFrame(results)\n",
    "perm_df.to_csv(\"out/results_perm.csv\")\n",
    "print(perm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee725236",
   "metadata": {},
   "source": [
    "### Generate a figure with histograms to visualize all of the above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5), sharex=\"all\", squeeze=True)\n",
    "\n",
    "# AD Score\n",
    "axes[0].hist(\n",
    "    perm_df[\"AD Score\"], bins=30, color=\"skyblue\", edgecolor=\"black\", linewidth=0.5\n",
    ")\n",
    "axes[0].axvline(\n",
    "    score_ad, color=\"red\", linestyle=\"--\", label=f\"True Score = {score_ad:.2f}\"\n",
    ")\n",
    "axes[0].set_title(\"AD/HC Score Distribution\")\n",
    "axes[0].set_xlabel(\"Score\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "# TBI Score\n",
    "axes[1].hist(\n",
    "    perm_df[\"TBI Score\"], bins=30, color=\"lightgreen\", edgecolor=\"black\", linewidth=0.5\n",
    ")\n",
    "axes[1].axvline(\n",
    "    score_tbi, color=\"red\", linestyle=\"--\", label=f\"True Score = {score_tbi:.2f}\"\n",
    ")\n",
    "axes[1].set_title(\"TBI+/TBI- Score Distribution\")\n",
    "axes[1].set_xlabel(\"Score\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "# Parameter-only TBI Score\n",
    "axes[2].hist(\n",
    "    perm_df[\"AD Param-only Score\"],\n",
    "    bins=30,\n",
    "    color=\"darkgreen\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "axes[2].axvline(\n",
    "    score_param_tbi,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"True Score = {score_param_tbi:.2f}\",\n",
    ")\n",
    "axes[2].set_title(\"Parameter-only TBI Score Distribution\")\n",
    "axes[2].set_xlabel(\"Score\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "axes[2].set_xlim(0, 1)\n",
    "axes[2].grid(True)\n",
    "axes[2].legend()\n",
    "\n",
    "# Transfer Score\n",
    "# axes[3].hist(perm_df[\"Transfer Score\"], bins=30, color='plum', edgecolor='black')\n",
    "# axes[3].axvline(score_transfer, color='red', linestyle='--', label=f\"True Score = {score_transfer:.2f}\")\n",
    "# axes[3].set_title(\"Transfer Score Distribution\")\n",
    "# axes[3].set_xlabel(\"Score\")\n",
    "# axes[3].set_ylabel(\"Frequency\")\n",
    "# axes[3].set_xlim(0,1)\n",
    "# axes[3].grid(True)\n",
    "# axes[3].legend()\n",
    "\n",
    "# Transfer Scaled Score\n",
    "axes[3].hist(\n",
    "    perm_df[\"Scaled Transfer Score\"],\n",
    "    bins=30,\n",
    "    color=\"purple\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "axes[3].axvline(\n",
    "    score_transfer_scaled,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"True Score = {score_transfer_scaled:.2f}\",\n",
    ")\n",
    "axes[3].set_title(\"Transfer Score Distribution\")\n",
    "axes[3].set_xlabel(\"Score\")\n",
    "axes[3].set_ylabel(\"Frequency\")\n",
    "axes[3].set_xlim(0, 1)\n",
    "axes[3].grid(True)\n",
    "axes[3].legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a normal distribution\n",
    "samples = np.random.normal(loc=0.5, scale=0.1, size=1000)\n",
    "\n",
    "# Rescale to fit the range [0.1, 0.9]\n",
    "samples = np.clip(samples, 0.1, 0.9)\n",
    "\n",
    "# Plot the results\n",
    "plt.hist(samples, bins=30, density=True, alpha=0.6, color=\"b\")\n",
    "plt.title(\"Rescaled Normal Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
