{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data-Setup\n",
    "\n",
    "Aim to run through the entire notebook in order to generate the appropriate pickle file which will be used in later notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.graph_theory import append_connectome_data, append_gt_data\n",
    "from scipy.signal import resample\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.vectors import FloatVector, StrVector\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "MCI_IS_AD = True\n",
    "NULL_MODEL = False\n",
    "DO_HARMONY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Subject Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + \"/data\"\n",
    "subdirectories = re.compile(r\"^(ADNI|MUSC|TBI|MCI)-(.*)-n\\d+$\")\n",
    "\n",
    "subject_info_df = pd.read_csv(data_path + \"/subjects.csv\")\n",
    "subject_info_df[\"Subject_ID\"] = subject_info_df[\"Subject\"].str.extract(r\"(\\d+)$\")\n",
    "subject_info_df[\"Site\"] = subject_info_df[\"Subject\"].str.extract(r\"^(\\d+)_\")\n",
    "\n",
    "\n",
    "# MUSC site is 999\n",
    "subject_info_df.loc[\n",
    "    (subject_info_df[\"Site\"].isna()) & (subject_info_df[\"Study\"] == \"C4D\"), \"Site\"\n",
    "] = 999\n",
    "subject_info_df.loc[\n",
    "    (subject_info_df[\"Site\"].isna()) & (subject_info_df[\"Study\"] == \"IAM\"), \"Site\"\n",
    "] = 999\n",
    "subject_info_df.loc[\n",
    "    (subject_info_df[\"Site\"].isna()) & (subject_info_df[\"Study\"] == \"MUSC\"), \"Site\"\n",
    "] = 999\n",
    "subject_info_df.loc[\n",
    "    (subject_info_df[\"Site\"].isna()) & (subject_info_df[\"Study\"] == \"DOD\"), \"Site\"\n",
    "] = 888\n",
    "\n",
    "if MCI_IS_AD:\n",
    "    # Treat MCI as AD for classification\n",
    "    subject_info_df.loc[subject_info_df[\"Diagnosis\"] == \"MCI\", \"Diagnosis\"] = \"AD\"\n",
    "\n",
    "print(subject_info_df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Subject Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory has TBI/MUSC/ADNI subdirectories, POS/NEG each\n",
    "# if \"data_df\" not in locals():\n",
    "columns = [\"Subject_ID\", \"Diagnosis\", \"Study\", \"Data\"]\n",
    "data_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    match = subdirectories.search(os.path.basename(root))\n",
    "    if match:\n",
    "        study = match.group(1)\n",
    "        diagnosis = match.group(2)\n",
    "        if MCI_IS_AD and diagnosis == \"MCI\":\n",
    "            diagnosis = \"AD\"\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\") and file != \"subjects.csv\":\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                subject_id_match = re.search(r\"(\\d+).tts_all\\.csv\", file)\n",
    "                if subject_id_match:\n",
    "                    subject_id = (\n",
    "                        subject_id_match.group(1).replace(\"_\", \"\").replace(\"-\", \"\")\n",
    "                    )\n",
    "\n",
    "                    # Smallest n observations is 140\n",
    "                    data_matrix = pd.read_csv(file_path).values\n",
    "                    if data_matrix.shape[0] > 140:\n",
    "                        data_matrix = resample(data_matrix, 140)\n",
    "\n",
    "                    # Find the row associated with this subject in subject_info_df\n",
    "                    # Extract Age (yrs) and Sex\n",
    "                    # Lookup by Subject_ID column\n",
    "                    # Append this information to new_row as \"Age\" and \"Sex\"\n",
    "                    subject_row = subject_info_df.loc[\n",
    "                        subject_info_df[\"Subject_ID\"] == subject_id\n",
    "                    ]\n",
    "                    age = (\n",
    "                        subject_row[\"Age (yrs)\"].values[0]\n",
    "                        if not subject_row.empty\n",
    "                        else None\n",
    "                    )\n",
    "                    sex = (\n",
    "                        subject_row[\"Sex\"].values[0] if not subject_row.empty else None\n",
    "                    )\n",
    "                    site = (\n",
    "                        subject_row[\"Site\"].values[0] if not subject_row.empty else None\n",
    "                    )\n",
    "\n",
    "                    new_row = {\n",
    "                        \"Subject_ID\": subject_id,\n",
    "                        \"Diagnosis\": diagnosis,\n",
    "                        \"Study\": study,\n",
    "                        \"Age\": age,\n",
    "                        \"Sex\": sex,\n",
    "                        \"Site\": site,\n",
    "                        \"Data\": data_matrix,\n",
    "                    }\n",
    "                    data_df = pd.concat(\n",
    "                        [data_df, pd.DataFrame([new_row])], ignore_index=True\n",
    "                    )\n",
    "data_df = append_connectome_data(data_df)\n",
    "print(\n",
    "    data_df.loc[\n",
    "        :, (data_df.columns != \"Data\") & (data_df.columns != \"Connectome\")\n",
    "    ].to_markdown()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap Sites based on unique incremental\n",
    "site_ids = data_df[\"Site\"].unique()\n",
    "site_ids_mapping = {}\n",
    "for id in site_ids:\n",
    "    site_ids_mapping[id] = len(site_ids_mapping)\n",
    "data_df[\"Site\"] = data_df[\"Site\"].map(site_ids_mapping)\n",
    "data_df[\"Site\"] = data_df[\"Site\"].values.astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonize those sites with more than 2 subjects\n",
    "site_counts = data_df.loc[data_df[\"Site\"] != 45, \"Site\"].value_counts()\n",
    "valid_sites = site_counts[site_counts > 2].index\n",
    "harmonize_data = data_df[\n",
    "    (data_df[\"Site\"].isin(valid_sites)) & (data_df[\"Site\"] != 45)\n",
    "].copy()\n",
    "harmonize_data[\"Site\"] = harmonize_data[\"Site\"].astype(str)\n",
    "\n",
    "\n",
    "# Unpack connectome edges into a vector\n",
    "def upper_triangle_flatten(matrix):\n",
    "    \"\"\"Extracts the upper triangle (excluding diagonal) and flattens it.\"\"\"\n",
    "    return matrix[np.triu_indices(matrix.shape[0], k=1)]\n",
    "\n",
    "\n",
    "n_features = harmonize_data[\"Connectome\"][0].shape[1]\n",
    "harmonize_data[\"Connectome\"] = harmonize_data[\"Connectome\"].apply(\n",
    "    upper_triangle_flatten\n",
    ")\n",
    "connectome_matrix = np.vstack(harmonize_data[\"Connectome\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_HARMONY:\n",
    "    # pandas2ri.activate()\n",
    "    comfam_path = \"./src/R/comfam.R\"\n",
    "    ro.r[\"source\"](comfam_path)\n",
    "\n",
    "    comfam = ro.globalenv[\"comfam\"]\n",
    "\n",
    "    num_subjects, num_features = connectome_matrix.shape\n",
    "\n",
    "    connectome_r = ro.r.matrix(\n",
    "        FloatVector(connectome_matrix.flatten()),\n",
    "        nrow=num_subjects,\n",
    "        ncol=num_features,\n",
    "        byrow=True,\n",
    "    )\n",
    "    site_r = ro.r[\"factor\"](StrVector(harmonize_data[\"Site\"].astype(str).values))\n",
    "\n",
    "    age_r = FloatVector(harmonize_data[\"Age\"].astype(float).values)\n",
    "    sex_r = ro.r[\"factor\"](StrVector(harmonize_data[\"Sex\"].astype(str).values))\n",
    "    covar_df_r = ro.DataFrame({\"Age\": age_r, \"Sex\": sex_r})\n",
    "    formula_r = ro.r(\"y ~ Age + Sex\")\n",
    "\n",
    "    comfam = ro.globalenv[\"comfam\"]\n",
    "    com_out = comfam(\n",
    "        connectome_r, site_r, covar=covar_df_r, model=ro.r[\"lm\"], formula=formula_r\n",
    "    )\n",
    "\n",
    "    # Apply weights to TBI data\n",
    "    tbi_data = data_df[data_df[\"Site\"] == 45].copy()\n",
    "    tbi_data[\"Site\"] = tbi_data[\"Site\"].astype(str)\n",
    "    tbi_data[\"Connectome\"] = tbi_data[\"Connectome\"].apply(upper_triangle_flatten)\n",
    "    tbi_connectome_matrix = np.vstack(tbi_data[\"Connectome\"].values)\n",
    "\n",
    "    num_tbi_subjects, num_features = tbi_connectome_matrix.shape\n",
    "    tbi_connectome_r = ro.r.matrix(\n",
    "        FloatVector(tbi_connectome_matrix.flatten()),\n",
    "        nrow=num_tbi_subjects,\n",
    "        ncol=num_features,\n",
    "        byrow=True,\n",
    "    )\n",
    "    tbi_site_r = ro.r[\"factor\"](StrVector(tbi_data[\"Site\"].astype(str).values))\n",
    "\n",
    "    age_r = FloatVector(tbi_data[\"Age\"].astype(float).values)\n",
    "    sex_r = ro.r[\"factor\"](StrVector(tbi_data[\"Sex\"].astype(str).values))\n",
    "    covar_df_r = ro.DataFrame({\"Age\": age_r, \"Sex\": sex_r})\n",
    "\n",
    "    comfam_predict = ro.r[\"predict\"]\n",
    "    tbi_harmonized_r = comfam_predict(\n",
    "        com_out, tbi_connectome_r, tbi_site_r, newcovar=covar_df_r\n",
    "    )\n",
    "    tbi_harmonized = np.array(tbi_harmonized_r.rx2(\"dat.combat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_connectome(vector, size=n_features):\n",
    "    \"\"\"Reconstructs a full connectome from a flattened upper triangle vector.\"\"\"\n",
    "    matrix = np.zeros((size, size))\n",
    "    upper_indices = np.triu_indices(size, k=1)\n",
    "    matrix[upper_indices] = vector\n",
    "    matrix += matrix.T\n",
    "    return matrix\n",
    "\n",
    "\n",
    "if DO_HARMONY:\n",
    "    harmonized_connectome = np.array(com_out.rx2(\"dat.combat\"))\n",
    "else:\n",
    "    harmonized_connectome = harmonize_data[\"Connectome\"]\n",
    "\n",
    "harmonize_data[\"R_Comfam\"] = [vec for vec in harmonized_connectome]\n",
    "harmonize_data.to_pickle(\"./data/harmonize_data.pkl\")\n",
    "\n",
    "reconstructed_connectomes = [\n",
    "    reconstruct_connectome(vec) for vec in harmonized_connectome\n",
    "]\n",
    "harmonized_dict = dict(zip(harmonize_data[\"Subject_ID\"], reconstructed_connectomes))\n",
    "data_df[\"Harmonized\"] = data_df[\"Subject_ID\"].map(harmonized_dict)\n",
    "\n",
    "if DO_HARMONY:\n",
    "    reconstructed_tbi = [reconstruct_connectome(vec) for vec in tbi_harmonized]\n",
    "    tbi_dict = dict(zip(tbi_data[\"Subject_ID\"], reconstructed_tbi))\n",
    "    data_df[\"Harmonized\"] = data_df[\"Harmonized\"].combine_first(\n",
    "        data_df[\"Subject_ID\"].map(tbi_dict)\n",
    "    )\n",
    "\n",
    "data_df = append_gt_data(data_df, harmonized=True)\n",
    "clone_df = data_df.copy()\n",
    "\n",
    "\n",
    "if NULL_MODEL:\n",
    "    data_df[\"Diagnosis\"] = np.random.permutation(data_df[\"Diagnosis\"].values)\n",
    "    data_df[\"Study\"] = np.random.permutation(data_df[\"Study\"].values)\n",
    "\n",
    "shapes = [\n",
    "    x.shape if isinstance(x, np.ndarray) else None for x in clone_df[\"Harmonized\"]\n",
    "]\n",
    "clone_df[\"Harmonized_Shape\"] = shapes\n",
    "\n",
    "clone_df = clone_df.drop(\n",
    "    [\"Harmonized\", \"Connectome\", \"Data\", \"EVC\", \"CLU\", \"DIV\"], axis=1\n",
    ")\n",
    "print(clone_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all but significant nodes from the graph theory metrics\n",
    "# WRITES IN PLACE!\n",
    "# print(data_df['EVC'][0].shape)\n",
    "\n",
    "# indices_to_keep = [1, 4, 77, 80, 114, 160]\n",
    "\n",
    "# def filter_array(arr):\n",
    "#     return arr[indices_to_keep]\n",
    "\n",
    "# data_df[\"EVC\"] = data_df[\"EVC\"].apply(lambda x: filter_array(x) if x is not None else None)\n",
    "# data_df[\"CLU\"] = data_df[\"CLU\"].apply(lambda x: filter_array(x) if x is not None else None)\n",
    "# data_df[\"DIV\"] = data_df\n",
    "# [\"DIV\"].apply(lambda x: filter_array(x) if x is not None else None)\n",
    "\n",
    "# print(data_df['EVC'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_pickle(\"./data/data.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
