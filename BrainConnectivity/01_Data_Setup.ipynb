{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data-Setup\n",
    "\n",
    "Aim to run through the entire notebook in order to generate the appropriate pickle file which will be used in later notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.graph_theory import append_connectome_data, append_gt_data\n",
    "from scipy.signal import resample\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.vectors import FloatVector, StrVector\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "MCI_IS_AD = True\n",
    "NULL_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Subject Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + \"/data\"\n",
    "subdirectories = re.compile(r\"^(ADNI|MUSC|TBI|MCI)-(.*)-n\\d+$\")\n",
    "\n",
    "subject_info_df = pd.read_csv(data_path + \"/subjects.csv\")\n",
    "subject_info_df['Subject_ID'] = subject_info_df['Subject'].str.extract(r'(\\d+)$')\n",
    "subject_info_df['Site'] = subject_info_df['Subject'].str.extract(r'^(\\d+)_')\n",
    "\n",
    "\n",
    "# MUSC site is 999\n",
    "subject_info_df.loc[(subject_info_df['Site'].isna()) & (subject_info_df['Study'] == 'C4D'), 'Site'] = 999\n",
    "subject_info_df.loc[(subject_info_df['Site'].isna()) & (subject_info_df['Study'] == 'IAM'), 'Site'] = 999\n",
    "subject_info_df.loc[(subject_info_df['Site'].isna()) & (subject_info_df['Study'] == 'MUSC'), 'Site'] = 999\n",
    "subject_info_df.loc[(subject_info_df['Site'].isna()) & (subject_info_df['Study'] == 'DOD'), 'Site'] = 888\n",
    "\n",
    "if MCI_IS_AD:\n",
    "    # Treat MCI as AD for classification\n",
    "    subject_info_df.loc[subject_info_df['Diagnosis'] == 'MCI', 'Diagnosis'] = 'AD'\n",
    "\n",
    "print(subject_info_df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Subject Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory has TBI/MUSC/ADNI subdirectories, POS/NEG each\n",
    "#if \"data_df\" not in locals():\n",
    "columns = [\"Subject_ID\", \"Diagnosis\", \"Study\", \"Data\"]\n",
    "data_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    match = subdirectories.search(os.path.basename(root))\n",
    "    if match:\n",
    "        study = match.group(1)\n",
    "        diagnosis = match.group(2)\n",
    "        if MCI_IS_AD and diagnosis == 'MCI':\n",
    "            diagnosis = 'AD'\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\") and file != \"subjects.csv\":\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                subject_id_match = re.search(r\"(\\d+).tts_all\\.csv\", file)\n",
    "                if subject_id_match:\n",
    "                    subject_id = subject_id_match.group(1).replace('_','').replace('-','')\n",
    "\n",
    "                    # Smallest n observations is 140\n",
    "                    data_matrix = pd.read_csv(file_path).values\n",
    "                    if data_matrix.shape[0] > 140:\n",
    "                        data_matrix = resample(data_matrix, 140)\n",
    "\n",
    "                    # Find the row associated with this subject in subject_info_df\n",
    "                    # Extract Age (yrs) and Sex\n",
    "                    # Lookup by Subject_ID column\n",
    "                    # Append this information to new_row as \"Age\" and \"Sex\"\n",
    "                    subject_row = subject_info_df.loc[subject_info_df['Subject_ID'] == subject_id]\n",
    "                    age = subject_row['Age (yrs)'].values[0] if not subject_row.empty else None\n",
    "                    sex = subject_row['Sex'].values[0] if not subject_row.empty else None\n",
    "                    site = subject_row['Site'].values[0] if not subject_row.empty else None\n",
    "\n",
    "                    new_row = {\n",
    "                        \"Subject_ID\": subject_id,\n",
    "                        \"Diagnosis\": diagnosis,\n",
    "                        \"Study\": study,\n",
    "                        \"Age\": age,\n",
    "                        \"Sex\": sex,\n",
    "                        \"Site\": site,\n",
    "                        \"Data\": data_matrix,\n",
    "                    }\n",
    "                    data_df = pd.concat(\n",
    "                        [data_df, pd.DataFrame([new_row])], ignore_index=True\n",
    "                        )\n",
    "data_df = append_connectome_data(data_df)\n",
    "print(data_df.loc[:, (data_df.columns != 'Data') & (data_df.columns != 'Connectome')].to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonize those sites with more than 2 subjects\n",
    "site_counts = data_df.loc[data_df['Site'] != 888, 'Site'].value_counts()\n",
    "valid_sites = site_counts[site_counts > 2].index\n",
    "harmonize_data = data_df[(data_df['Site'].isin(valid_sites)) & (data_df['Site'] != 888)].copy()\n",
    "harmonize_data['Site'] = harmonize_data['Site'].astype(str)\n",
    "\n",
    "# Unpack connectome edges into a vector\n",
    "def upper_triangle_flatten(matrix):\n",
    "    \"\"\"Extracts the upper triangle (excluding diagonal) and flattens it.\"\"\"\n",
    "    return matrix[np.triu_indices(matrix.shape[0], k=1)]\n",
    "\n",
    "n_features = harmonize_data['Connectome'][0].shape[1]\n",
    "harmonize_data['Connectome'] = harmonize_data['Connectome'].apply(upper_triangle_flatten)\n",
    "connectome_matrix = np.vstack(harmonize_data['Connectome'].values)\n",
    "harmonize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_HARMONY = False\n",
    "if DO_HARMONY:\n",
    "    pandas2ri.activate()\n",
    "    comfam_path = \"./src/R/comfam.R\"\n",
    "    ro.r['source'](comfam_path)\n",
    "\n",
    "    comfam = ro.globalenv['comfam']\n",
    "\n",
    "    num_subjects, num_features = connectome_matrix.shape\n",
    "\n",
    "    connectome_r = ro.r.matrix(FloatVector(connectome_matrix.flatten()), nrow=num_subjects, ncol=num_features, byrow=True)\n",
    "    site_r = ro.r['factor'](StrVector(harmonize_data['Site'].astype(str).values))\n",
    "\n",
    "    age_r = FloatVector(harmonize_data['Age'].astype(float).values)\n",
    "    sex_r = ro.r['factor'](StrVector(harmonize_data['Sex'].astype(str).values))\n",
    "    covar_df_r = ro.DataFrame({'Age': age_r, 'Sex': sex_r})\n",
    "    formula_r = ro.r('y ~ Age + Sex')\n",
    "\n",
    "    comfam = ro.globalenv['comfam']\n",
    "    com_out = comfam(connectome_r, site_r, covar=covar_df_r, model=ro.r['lm'], formula=formula_r)\n",
    "\n",
    "    # Apply weights to TBI data\n",
    "    tbi_data = data_df[data_df['Site'] == 888].copy()\n",
    "    tbi_data['Site'] = tbi_data['Site'].astype(str)\n",
    "    tbi_data['Connectome'] = tbi_data['Connectome'].apply(upper_triangle_flatten)\n",
    "    tbi_connectome_matrix = np.vstack(tbi_data['Connectome'].values)\n",
    "\n",
    "    num_tbi_subjects, num_features = tbi_connectome_matrix.shape\n",
    "    tbi_connectome_r = ro.r.matrix(FloatVector(tbi_connectome_matrix.flatten()), nrow=num_tbi_subjects, ncol=num_features, byrow=True)\n",
    "    tbi_site_r = ro.r['factor'](StrVector(tbi_data['Site'].astype(str).values))\n",
    "\n",
    "    age_r = FloatVector(tbi_data['Age'].astype(float).values)\n",
    "    sex_r = ro.r['factor'](StrVector(tbi_data['Sex'].astype(str).values))\n",
    "    covar_df_r = ro.DataFrame({'Age': age_r, 'Sex': sex_r})\n",
    "\n",
    "    comfam_predict = ro.r['predict']\n",
    "    tbi_harmonized_r = comfam_predict(com_out, tbi_connectome_r, tbi_site_r, newcovar=covar_df_r)\n",
    "    tbi_harmonized = np.array(tbi_harmonized_r.rx2('dat.combat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_connectome(vector, size=n_features):\n",
    "    \"\"\"Reconstructs a full connectome from a flattened upper triangle vector.\"\"\"\n",
    "    matrix = np.zeros((size, size))\n",
    "    upper_indices = np.triu_indices(size, k=1)\n",
    "    matrix[upper_indices] = vector\n",
    "    matrix += matrix.T\n",
    "    return matrix\n",
    "\n",
    "if DO_HARMONY:\n",
    "    harmonized_connectome = np.array(com_out.rx2('dat.combat'))\n",
    "else:\n",
    "    harmonized_connectome = harmonize_data[\"Connectome\"]\n",
    "\n",
    "reconstructed_connectomes = [reconstruct_connectome(vec) for vec in harmonized_connectome]\n",
    "harmonized_dict = dict(zip(harmonize_data['Subject_ID'], reconstructed_connectomes))\n",
    "data_df['Harmonized'] = data_df['Subject_ID'].map(harmonized_dict)\n",
    "\n",
    "if DO_HARMONY:\n",
    "    reconstructed_tbi = [reconstruct_connectome(vec) for vec in tbi_harmonized]\n",
    "    tbi_dict = dict(zip(tbi_data['Subject_ID'], reconstructed_tbi))\n",
    "    data_df['Harmonized'] = data_df['Harmonized'].combine_first(data_df['Subject_ID'].map(tbi_dict))\n",
    "\n",
    "data_df = append_gt_data(data_df, harmonized=True)\n",
    "\n",
    "if NULL_MODEL:\n",
    "    data_df['Diagnosis'] = np.random.permutation(data_df['Diagnosis'].values)\n",
    "    data_df['Study'] = np.random.permutation(data_df['Study'].values)\n",
    "\n",
    "clone_df = data_df.copy()\n",
    "shapes = [x.shape if isinstance(x, np.ndarray) else None for x in clone_df['Harmonized']]\n",
    "clone_df['Harmonized_Shape'] = shapes\n",
    "clone_df = clone_df.drop(['Harmonized', 'Connectome', 'Data', 'EVC', 'CLU', 'DIV'], axis=1)\n",
    "\n",
    "print(clone_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all but significant nodes from the graph theory metrics\n",
    "# WRITES IN PLACE!\n",
    "# print(data_df['EVC'][0].shape)\n",
    "\n",
    "# indices_to_keep = [1, 4, 77, 80, 114, 160]\n",
    "\n",
    "# def filter_array(arr):\n",
    "#     return arr[indices_to_keep]\n",
    "\n",
    "# data_df[\"EVC\"] = data_df[\"EVC\"].apply(lambda x: filter_array(x) if x is not None else None)\n",
    "# data_df[\"CLU\"] = data_df[\"CLU\"].apply(lambda x: filter_array(x) if x is not None else None)\n",
    "# data_df[\"DIV\"] = data_df\n",
    "# [\"DIV\"].apply(lambda x: filter_array(x) if x is not None else None)\n",
    "\n",
    "# print(data_df['EVC'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_pickle('./data/data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
